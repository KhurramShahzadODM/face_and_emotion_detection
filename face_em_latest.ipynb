{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_em_latest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "rm -r /content/face_and_emotion_detection"
      ],
      "metadata": {
        "id": "o8voE0CfuvZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the github **repository**"
      ],
      "metadata": {
        "id": "I4cjlU2F_yyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KhurramShahzadODM/face_and_emotion_detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UnuxLEwuR5t",
        "outputId": "1e7821b8-a6c5-4999-df5a-62d13e095bcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face_and_emotion_detection'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 57 (delta 10), reused 48 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k0f-VxaALy8",
        "outputId": "5abdbcd6-4d5c-4bf7-cdfb-668f89d1ce67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 22 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566185 sha256=f2d39d3d02e4d05972aead280a2c20b3f370490065a844372b60a8f784667ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "# clean and rebuild the image folders\n",
        "input_folder = '/content/face_and_emotion_detection/inputs'\n",
        "if os.path.exists(input_folder):\n",
        "  shutil.rmtree(input_folder)\n",
        "os.makedirs(input_folder)\n",
        "\n",
        "output_folder = '/content/face_and_emotion_detection/outputs'\n",
        "if os.path.exists(output_folder):\n",
        "  shutil.rmtree(output_folder)\n",
        "os.makedirs(output_folder)\n",
        "\n",
        "# upload images (PNG or JPG)\n",
        "image_names = list(files.upload().keys())\n",
        "for image_name in image_names:\n",
        "  shutil.move(image_name, os.path.join(input_folder, image_name))\n"
      ],
      "metadata": {
        "id": "FoSUHIFL4iJX",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bc94a9eb-c915-43aa-95b8-bcfd328878ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f1173cd4-864a-4f94-95ca-7c996e7dad1d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f1173cd4-864a-4f94-95ca-7c996e7dad1d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trump_speech_short.mp4 to trump_speech_short.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import face_recognition\n",
        "from keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imutils\n",
        "import keras\n",
        "\n",
        "\n",
        "imagePaths = list(paths.list_images('/content/face_and_emotion_detection/trump'))\n",
        "# initialize the list of known encodings and known names\n",
        "knownEncodings = []\n",
        "knownNames = []\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "  # extract the person name from the image path\n",
        "  print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
        "    len(imagePaths)))\n",
        "  name = imagePath.split(os.path.sep)[-2]\n",
        "  # load the input image and convert it from BGR (OpenCV ordering)\n",
        "  # to dlib ordering (RGB)\n",
        "  image = cv2.imread(imagePath)\n",
        "  rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  # rgb = cv2.resize(rgb, (512,640))\n",
        "  rgb = imutils.resize(rgb, width=750)\n",
        "  boxes = face_recognition.face_locations(rgb,model=\"cnn\")\n",
        "  encodings = face_recognition.face_encodings(rgb, boxes)\n",
        "  for encoding in encodings:\n",
        "    # add each encoding + name to our set of known names and\n",
        "    # encodings\n",
        "    knownEncodings.append(encoding)\n",
        "    knownNames.append(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUBCVUwEk4I3",
        "outputId": "d67e9721-5742-44d5-cfbb-7769c4e55004"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] processing image 1/11\n",
            "[INFO] processing image 2/11\n",
            "[INFO] processing image 3/11\n",
            "[INFO] processing image 4/11\n",
            "[INFO] processing image 5/11\n",
            "[INFO] processing image 6/11\n",
            "[INFO] processing image 7/11\n",
            "[INFO] processing image 8/11\n",
            "[INFO] processing image 9/11\n",
            "[INFO] processing image 10/11\n",
            "[INFO] processing image 11/11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.models import load_model\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# rows, cols = (7, 1)\n",
        "# results = [[0]*cols]*rows\n",
        "results = [[] for i in range(7)]\n",
        "print(results)\n",
        "# Create a VideoCapture object and read from input file\n",
        "# If the input is the camera, pass 0 instead of the video file name\n",
        "video_path = \"/content/face_and_emotion_detection/inputs/trump_1min_sec.mp4\"\n",
        "# video_path = \"/content/face_and_emotion_detection/inputs/trump-1mint.mp4\"\n",
        "\n",
        "File_name = os.path.basename(video_path)\n",
        "\n",
        "# cap = cv2.VideoCapture(video_path)\n",
        "emotion_dict= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}\n",
        "\n",
        "total_frame_wd_face = 0\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened()== False): \n",
        "  print(\"Error opening video stream or file\")\n",
        "# img = cv2.imread('/content/face_and_emotion_detection/inputs/trump.jpg', cv2.IMREAD_COLOR)\n",
        "\n",
        "# cv2_imshow(img)\n",
        "# Read until video is completed\n",
        "# while(cap.isOpened()):\n",
        "while(total_frame_wd_face < 1):\n",
        "  # print(total_frame_wd_face)\n",
        "  # Capture frame-by-frame\n",
        "  ret, frame = cap.read()\n",
        "  if ret == True:\n",
        "    # Display the resulting frame\n",
        "    # cv2_imshow(frame)\n",
        "    # frame = img\n",
        "    # frame = cv2.resize(frame, (512,640))\n",
        "    frame = imutils.resize(frame, width=750)\n",
        "\n",
        "    faces = face_recognition.face_locations(frame,model=\"cnn\")\n",
        "    # print(len(faces))\n",
        "    # plt.imshow(face_image1)\n",
        "    trump_face = 0\n",
        "    for face in faces:\n",
        "      # print(face)\n",
        "      encoding_2 = face_recognition.face_encodings(frame, [face])\n",
        "      matches = face_recognition.compare_faces(knownEncodings,encoding)\n",
        "      # print(matches)\n",
        "      # print(len(knownEncodings))\n",
        "      # print(sum(matches))\n",
        "      if sum(matches) > 0:\n",
        "        trump_face = face\n",
        "        break\n",
        "    # print(trump_face)\n",
        "    if(trump_face != 0):\n",
        "      total_frame_wd_face += 1\n",
        "      top, right, bottom, left = trump_face\n",
        "      face_image = frame[top:bottom, left:right]\n",
        "      face_image = cv2.resize(face_image, (48,48))\n",
        "      # cv2_imshow(face_image)\n",
        "      face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
        "      face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])\n",
        "     \n",
        "      # y_1 = model_1.predict(face_image)\n",
        "\n",
        "      # l = np.argmax(y_1, axis=1)\n",
        "      # print('conf : '+ str(format(y_1[0][l[0]],'f')))\n",
        "      # print('label : ' + str(emotions[l[0]][0]))\n",
        "      \n",
        "      model = load_model(\"/content/face_and_emotion_detection/model/model_v6_23.hdf5\")\n",
        "      result = model.predict(face_image)\n",
        "      # print(result)\n",
        "      predicted_class = np.argmax(result)\n",
        "      # label_map = dict((v,k) for k,v in emotion_dict.items())\n",
        "      # print(label_map[predicted_class])\n",
        "      # print(predicted_class)\n",
        "      confidence = format(result[0][predicted_class],'f')\n",
        "      # print(confidence)\n",
        "      # print(predicted_class)\n",
        "      results[predicted_class].append(confidence)\n",
        "      # print(predicted_label)\n",
        "\n",
        "\n",
        "      # results = face_recognition.compare_faces(knownEncodings, encoding_2,tolerance=0.5)\n",
        "      # print(results)\n",
        "      # if(results[0]):\n",
        "      #   print('trump is in the frame')\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "  # Break the loop\n",
        "  else: \n",
        "    break\n",
        "\n",
        "# When everything done, release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n",
        "label_map = dict((v,k) for k,v in emotion_dict.items())\n",
        "# for i,result in results:\n",
        "f = open(\"/content/face_and_emotion_detection/results.txt\", \"a\")\n",
        "File_name\n",
        "f.write(str(File_name + \"\\n\"))\n",
        "f.write(\"label, Percent \\n\")\n",
        "\n",
        "print (\"label, Percent\")\n",
        "for idx, result in enumerate(results):\n",
        "    # print(result)\n",
        "    result = np.array(result).astype(np.float)\n",
        "    print(label_map[idx], \"  \" , int(np.sum(result))/total_frame_wd_face*100)\n",
        "    f.write(str(label_map[idx] + \"  \" + str(int(np.sum(result))/total_frame_wd_face*100)) + \"\\n\")\n",
        "f.write(\"\\n\")\n",
        "f.close()\n",
        "  # predicted_label = label_map[predicted_class]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W4LFkeMbHDq",
        "outputId": "2d3d1e7f-c0d7-4ee9-e555-a1fed62e8fb6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], [], [], [], [], [], []]\n",
            "label, Percent\n",
            "Angry    0.0\n",
            "Disgust    0.0\n",
            "Fear    100.0\n",
            "Happy    0.0\n",
            "Neutral    0.0\n",
            "Sad    0.0\n",
            "Surprise    0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_file = open(\"/content/face_and_emotion_detection/results.txt\")\n",
        "lines = a_file. readlines()\n",
        "for line in lines:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELNGJkte9UKc",
        "outputId": "2ec39d22-e99f-4316-8fc7-64eee660da66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trump_1min_sec.mp4\n",
            "\n",
            "label, Percent \n",
            "\n",
            "Angry  0.0\n",
            "\n",
            "Disgust  0.0\n",
            "\n",
            "Fear  100.0\n",
            "\n",
            "Happy  0.0\n",
            "\n",
            "Neutral  0.0\n",
            "\n",
            "Sad  0.0\n",
            "\n",
            "Surprise  0.0\n",
            "\n",
            "\n",
            "\n",
            "trump_1min_sec.mp4\n",
            "\n",
            "label, Percent \n",
            "\n",
            "Angry  0.0\n",
            "\n",
            "Disgust  0.0\n",
            "\n",
            "Fear  100.0\n",
            "\n",
            "Happy  0.0\n",
            "\n",
            "Neutral  0.0\n",
            "\n",
            "Sad  0.0\n",
            "\n",
            "Surprise  0.0\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/face.zip /content/face_and_emotion_detection/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jv8gzGD3Acb",
        "outputId": "097acd95-7573-45da-d5aa-7d59f0bfe87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/face_and_emotion_detection/inputs/ (stored 0%)\n",
            "  adding: content/face_and_emotion_detection/LICENSE (deflated 41%)\n",
            "  adding: content/face_and_emotion_detection/model/ (stored 0%)\n",
            "  adding: content/face_and_emotion_detection/outputs/ (stored 0%)\n",
            "  adding: content/face_and_emotion_detection/README.md (deflated 4%)\n",
            "  adding: content/face_and_emotion_detection/test_images/ (stored 0%)\n",
            "  adding: content/face_and_emotion_detection/trump/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "emotions = {\n",
        "    0: ['Angry', (0,0,255), (255,255,255)],\n",
        "    1: ['Disgust', (0,102,0), (255,255,255)],\n",
        "    2: ['Fear', (255,255,153), (0,51,51)],\n",
        "    3: ['Happy', (153,0,153), (255,255,255)],\n",
        "    4: ['Sad', (255,0,0), (255,255,255)],\n",
        "    5: ['Surprise', (0,255,0), (255,255,255)],\n",
        "    6: ['Neutral', (160,160,160), (255,255,255)]\n",
        "}\n",
        "num_classes = len(emotions)\n",
        "input_shape = (48, 48, 1)\n",
        "weights_1 = '/content/face_and_emotion_detection/model/vggnet.h5'"
      ],
      "metadata": {
        "id": "o5XhAnUAbxeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNet(Sequential):\n",
        "    def __init__(self, input_shape, num_classes, checkpoint_path, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.add(Rescaling(1./255, input_shape=input_shape))\n",
        "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(MaxPool2D())\n",
        "        self.add(Dropout(0.5))\n",
        "\n",
        "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(MaxPool2D())\n",
        "        self.add(Dropout(0.4))\n",
        "\n",
        "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(MaxPool2D())\n",
        "        self.add(Dropout(0.5))\n",
        "\n",
        "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
        "        self.add(BatchNormalization())\n",
        "        self.add(MaxPool2D())\n",
        "        self.add(Dropout(0.4))\n",
        "\n",
        "        self.add(Flatten())\n",
        "        \n",
        "        self.add(Dense(1024, activation='relu'))\n",
        "        self.add(Dropout(0.5))\n",
        "        self.add(Dense(256, activation='relu'))\n",
        "\n",
        "        self.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        self.compile(optimizer=Adam(learning_rate=lr),\n",
        "                    loss=categorical_crossentropy,\n",
        "                    metrics=['accuracy'])\n",
        "        \n",
        "        self.checkpoint_path = checkpoint_path\n",
        "model_1 = VGGNet(input_shape, num_classes, weights_1)\n",
        "model_1.load_weights(model_1.checkpoint_path)"
      ],
      "metadata": {
        "id": "0RDXnG5ab8Qq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}